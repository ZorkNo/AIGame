{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ImageTextCompCLIP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMrguj0KbW82pLndxsRfYMY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZorkNo/AIGame/blob/master/ImageTextCompCLIP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4W_NShwkDPQ",
        "outputId": "db0792e7-2855-4d01-86b6-dc9f90708fb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting clip-by-openai\n",
            "  Downloading clip_by_openai-1.1-py3-none-any.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from clip-by-openai) (4.64.0)\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 2.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from clip-by-openai) (2022.6.2)\n",
            "Collecting torchvision==0.8.2\n",
            "  Downloading torchvision-0.8.2-cp37-cp37m-manylinux1_x86_64.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 17.3 MB/s \n",
            "\u001b[?25hCollecting torch<1.7.2,>=1.7.1\n",
            "  Downloading torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 776.8 MB 17 kB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.8.2->clip-by-openai) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==0.8.2->clip-by-openai) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<1.7.2,>=1.7.1->clip-by-openai) (4.1.1)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->clip-by-openai) (0.2.5)\n",
            "Installing collected packages: torch, torchvision, ftfy, clip-by-openai\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.1+cu113\n",
            "    Uninstalling torch-1.12.1+cu113:\n",
            "      Successfully uninstalled torch-1.12.1+cu113\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.13.1+cu113\n",
            "    Uninstalling torchvision-0.13.1+cu113:\n",
            "      Successfully uninstalled torchvision-0.13.1+cu113\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.7.1 which is incompatible.\n",
            "torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "Successfully installed clip-by-openai-1.1 ftfy-6.1.1 torch-1.7.1 torchvision-0.8.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.24.51-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 4.2 MB/s \n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting botocore<1.28.0,>=1.27.51\n",
            "  Downloading botocore-1.27.51-py3-none-any.whl (9.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.0 MB 28.8 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 10.7 MB/s \n",
            "\u001b[?25hCollecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.11-py2.py3-none-any.whl (139 kB)\n",
            "\u001b[K     |████████████████████████████████| 139 kB 70.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.28.0,>=1.27.51->boto3) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.28.0,>=1.27.51->boto3) (1.15.0)\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.11 which is incompatible.\u001b[0m\n",
            "Successfully installed boto3-1.24.51 botocore-1.27.51 jmespath-1.0.1 s3transfer-0.6.0 urllib3-1.26.11\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting colab-env\n",
            "  Downloading colab-env-0.2.0.tar.gz (4.7 kB)\n",
            "Collecting python-dotenv<1.0,>=0.10.0\n",
            "  Downloading python_dotenv-0.20.0-py3-none-any.whl (17 kB)\n",
            "Building wheels for collected packages: colab-env\n",
            "  Building wheel for colab-env (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for colab-env: filename=colab_env-0.2.0-py3-none-any.whl size=3838 sha256=3d18659f7e3ae18724a55ce428565d0b37be15f917134fb81d2cd0b11cb747b3\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/ca/e8/3d25b6abb4ac719ecb9e837bb75f2a9b980430005fb12a9107\n",
            "Successfully built colab-env\n",
            "Installing collected packages: python-dotenv, colab-env\n",
            "Successfully installed colab-env-0.2.0 python-dotenv-0.20.0\n"
          ]
        }
      ],
      "source": [
        "#CLIP\n",
        "!pip install clip-by-openai\n",
        "#AWS \n",
        "!pip install boto3\n",
        "\n",
        "## more sacure way to save passwords\n",
        "!pip install colab-env --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opensearch-py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJXude7hZoCv",
        "outputId": "883aac09-29e6-483e-e341-a1c4086d5759"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting opensearch-py\n",
            "  Downloading opensearch_py-2.0.0-py2.py3-none-any.whl (204 kB)\n",
            "\u001b[K     |████████████████████████████████| 204 kB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from opensearch-py) (2.23.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from opensearch-py) (2022.6.15)\n",
            "Requirement already satisfied: urllib3<2,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from opensearch-py) (1.26.11)\n",
            "Collecting urllib3<2,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 38.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.4.0->opensearch-py) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.4.0->opensearch-py) (2.10)\n",
            "Installing collected packages: urllib3, opensearch-py\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.26.11\n",
            "    Uninstalling urllib3-1.26.11:\n",
            "      Successfully uninstalled urllib3-1.26.11\n",
            "Successfully installed opensearch-py-2.0.0 urllib3-1.25.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import clip\n",
        "\n",
        "import cv2\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import boto3\n",
        "\n",
        "import colab_env\n",
        "import os\n",
        "\n",
        "import datetime\n",
        "from opensearchpy import OpenSearch, helpers\n",
        "import click"
      ],
      "metadata": {
        "id": "aNGQWL8ykHZU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e1e4250-6b46-42d1-ec88-ea404e9adf4c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model_name = \"ViT-B/32\"\n",
        "print(clip.available_models())\n",
        "print('Loading clip model...')\n",
        "model, preprocess = clip.load(model_name, device=device)\n",
        "print('done.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yK4EL19BkI1r",
        "outputId": "041cf0f2-7929-4d33-9357-f6d54454b7ae"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['RN50', 'ViT-B/32']\n",
            "Loading clip model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████████████████| 353976522/353976522 [00:02<00:00, 167636092.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi -L"
      ],
      "metadata": {
        "id": "T88FU0nmaeXV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fcc2116-d9e9-4c94-ab7c-4f976b44314e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla P100-PCIE-16GB (UUID: GPU-69c4c91c-4fc8-29b2-8cab-8e60a5fe35a1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def makedir(path):\n",
        "  if os.path.exists(path) == False :\n",
        "    os.mkdir(path)\n",
        "\n",
        "def save_image(parent_dir,dir,image_file_name,cvimage):\n",
        "  #make directories\n",
        "  makedir('/'+parent_dir+'/')\n",
        "  makedir('/'+parent_dir+'/'+dir+'/')\n",
        "  #saving file\n",
        "  return cv2.imwrite('/'+parent_dir+'/'+dir+'/'+image_file_name+'',cvimage)\n",
        "\n",
        "s3 = boto3.resource('s3')\n",
        "bucket = s3.Bucket('vframes')\n",
        "\n",
        "def upload(parent_dir,dir,image_file_name,bucketname,cvimage):\n",
        "  save_image(parent_dir,dir,image_file_name,cvimage)\n",
        "\n",
        "  path = os.path.join('/',parent_dir,dir,image_file_name)\n",
        "  s3path = os.path.join(parent_dir,dir,image_file_name)\n",
        "  print ((path))\n",
        "  #print (os.path.exists(path))\n",
        "  s3.meta.client.upload_file(path,bucketname,s3path)"
      ],
      "metadata": {
        "id": "LIp7QlynYrcU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function that computes the feature vectors for a batch of images\n",
        "def compute_clip_features(photos):\n",
        "    \n",
        "    # Preprocess all photos\n",
        "    photos_preprocessed = torch.stack([preprocess(photo) for photo in photos]).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Encode the photos batch to compute the feature vectors and normalize them\n",
        "        photos_features = model.encode_image(photos_preprocessed)\n",
        "        photos_features /= photos_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    # Transfer the feature vectors back to the CPU and convert to numpy\n",
        "    return photos_features.cpu().numpy()"
      ],
      "metadata": {
        "id": "EEwmEJ3Lh-qL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Function that convert from cv2 to pillow image\n",
        "def convert_from_cv2_to_image(img: np.ndarray) -> Image:\n",
        "    # return Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "    return Image.fromarray(img)\n",
        "\n",
        "def convert_from_image_to_cv2(img: Image) -> np.ndarray:\n",
        "    # return cv2.cvtColor(numpy.array(img), cv2.COLOR_RGB2BGR)\n",
        "    return np.asarray(img)"
      ],
      "metadata": {
        "id": "CemJB8d5iQJM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureTimespan:\n",
        "  def __init__(self, title, prdNo,url,sourceUrl,imageUrl,frameStart,frameEnd,featureFrame,features):\n",
        "    self.title = title\n",
        "    self.prdNo = prdNo\n",
        "    self.url = url\n",
        "    self.sourceUrl  = sourceUrl\n",
        "    self.imageUrl = imageUrl\n",
        "    self.frameStart  = frameStart\n",
        "    self.frameEnd  = frameEnd\n",
        "    self.featureFrame  = featureFrame\n",
        "    self.features = features"
      ],
      "metadata": {
        "id": "AdgD4HeydD1p"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "usr = os.getenv(\"AWS_OPENSEARCH_USR\")\n",
        "psw = os.getenv(\"AWS_OPENSEARCH_PSW\")\n",
        "\n",
        "def load_frames_in_index(featureTimespans, es_url, index_name):\n",
        "    es = OpenSearch(es_url, http_auth=(usr, psw), port=80, request_timeout=400,\n",
        "                       max_retries=10, retry_on_timeout=True, use_ssl = True)\n",
        "    now = datetime.datetime.utcnow()\n",
        "    length = len(featureTimespans)\n",
        "    actions = ({\n",
        "        \"_index\": index_name,\n",
        "        \"title\": features.title,\n",
        "        \"prdNo\": features.prdNo,\n",
        "        \"url\": features.url,\n",
        "        \"sourceUrl\": features.sourceUrl,\n",
        "        \"imageUrl\": features.imageUrl,\n",
        "        \"frameStart\": features.frameStart,\n",
        "        \"frameEnd\": features.frameEnd,\n",
        "        \"featureFrame\": features.featureFrame,\n",
        "        \"features\": features.features,\n",
        "        \"@timestamp\": now\n",
        "    } for features in featureTimespans)\n",
        "\n",
        "    loading = helpers.streaming_bulk(\n",
        "        client=es,\n",
        "        actions=actions,\n",
        "        max_retries=5\n",
        "    )\n",
        "    print(f'Loading {length} photos...', flush=True)\n",
        "    with click.progressbar(loading, length=length) as task:\n",
        "        for success, info in task:\n",
        "            if not success:\n",
        "                print('A document failed:', info)\n",
        "\n",
        "    print('Loaded photos')"
      ],
      "metadata": {
        "id": "KDT1dloZXk8t"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XBNDa6v6knAo"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import dependencies\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import time\n",
        "from scipy import spatial\n",
        "\n",
        "# Define the batch size so that it fits on your GPU. You can also do the processing on the CPU, but it will be slower.\n",
        "batch_size = 512\n",
        "# How many frames will be encode (1 / X frame_amount = 1 is all frames\n",
        "frame_amount = 4\n",
        "\n",
        "#\n",
        "administrativeTitle= 'Spise med Price S13:E2 Spise med Price - Soul Food [00652105020]'\n",
        "productionNumber= '00652105020'\n",
        "siteUrl= 'https://www.dr.dk/drtv/episode/spise-med-price_-soul-food_265992'\n",
        "streamUrl = 'https://drod22m.akamaized.net/all/clear/none/a8/6135f36faf5a611d6c7849a8/00652105020/stream_ts/master_manifest.m3u8'\n",
        "streamUrl2 ='https://drod22m.akamaized.net/all/clear/none/a8/6135f36faf5a611d6c7849a8/00652105020/stream_ts/Spise-med-Price---Soul_13520bccc26942fda22024a003ae2041_3500.m3u8'"
      ],
      "metadata": {
        "id": "_ELEpLyLn4hS"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cap = cv2.VideoCapture(streamUrl2)\n",
        "batchCount = 0\n",
        "frame_count = 0\n",
        "image_count = 0\n",
        "keyframe_count = 0\n",
        "images = []\n",
        "frame = []\n",
        "listfeatureTimespans =[]\n",
        "pre_image_feature = np.empty\n",
        "pre_image_feature_set = False\n",
        "#pre_image = Null\n",
        "pre_image_frame=1\n",
        "last_frame=False\n",
        "\n",
        "while cap.isOpened():    \n",
        "    frame_count = frame_count + 1 \n",
        "    ret, image = cap.read()\n",
        "\n",
        "    if not ret:\n",
        "      print('not ret') \n",
        "      last_frame =True\n",
        "      #break\n",
        "\n",
        "    #Only read every (frame_amount) frames\n",
        "    if frame_count%frame_amount  == 0 or last_frame :\n",
        "      #print ('f'+ str(frame_count)\n",
        "      if(last_frame==False): \n",
        "        image_count = image_count + 1\n",
        "        images.append(convert_from_cv2_to_image(image))\n",
        "        frame.append(frame_count)\n",
        "\n",
        "      # encode when reaches (batch_size) \n",
        "      if image_count%batch_size == 0 or (last_frame and len(images)>0):\n",
        "\n",
        "        print('Encode')\n",
        "        images_features = compute_clip_features(images)\n",
        "        print('Encoded')\n",
        "        j=0\n",
        "        \n",
        "        for imageF in images_features:\n",
        "\n",
        "          if pre_image_feature_set==False:\n",
        "            pre_image_frame=1\n",
        "            pre_image_cv = image[0]\n",
        "            pre_image_feature = imageF\n",
        "            pre_image_feature_set=True\n",
        "\n",
        "          result = 1 - spatial.distance.cosine(pre_image_feature, imageF)\n",
        "          if result <0.85:\n",
        "            objectname = str(pre_image_frame).zfill(6)+'.png'\n",
        "            \n",
        "            ##uploads\n",
        "            upload('frames',productionNumber,objectname,'vframes',pre_image_cv)\n",
        "\n",
        "            #writes to opensearch\n",
        "            featureTimespan = FeatureTimespan(\"\",\"\",\"\",\"\",\"\",0,0,0,np.zeros(512))\n",
        "            featureTimespan.title = administrativeTitle\n",
        "            featureTimespan.prdNo = productionNumber\n",
        "            featureTimespan.url = siteUrl\n",
        "            featureTimespan.sourceUrl = streamUrl\n",
        "            featureTimespan.imageUrl ='https://vframes.s3.eu-north-1.amazonaws.com/frames/'+productionNumber+'/'+objectname\n",
        "            featureTimespan.frameStart =pre_image_frame\n",
        "            featureTimespan.frameEnd = frame[j]-1\n",
        "            featureTimespan.featureFrame = pre_image_frame\n",
        "            featureTimespan.features = pre_image_feature\n",
        "            listfeatureTimespans.append(featureTimespan)\n",
        "            \n",
        "            pre_image_frame=frame[j]\n",
        "            pre_image_cv = convert_from_image_to_cv2(images[j])\n",
        "            pre_image_feature = imageF\n",
        "                    \n",
        "          j=j+1\n",
        "        load_frames_in_index(listfeatureTimespans,\"https://search-video-ovjjz53fbtmpdokttzlfxmkrpu.eu-west-2.es.amazonaws.com\",\"image_search\")\n",
        "        listfeatureTimespans= []\n",
        "        images = []\n",
        "        frame= []\n",
        "\n",
        "\n",
        "    #cv2.waitKey(1) & 0xff\n",
        "\n",
        "cv2.destroyAllWindows()\n",
        "cap.release()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-dubzkLi1ef",
        "outputId": "eaa06dcd-3b95-4813-cbc1-ffaa31feb0de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encode\n",
            "Encoded\n",
            "/frames/00652105020/000001.png\n",
            "/frames/00652105020/000200.png\n",
            "/frames/00652105020/000208.png\n",
            "/frames/00652105020/000220.png\n",
            "/frames/00652105020/000228.png\n",
            "/frames/00652105020/000244.png\n",
            "/frames/00652105020/000252.png\n",
            "/frames/00652105020/000256.png\n",
            "/frames/00652105020/000320.png\n",
            "/frames/00652105020/000332.png\n",
            "/frames/00652105020/000340.png\n",
            "/frames/00652105020/000356.png\n",
            "/frames/00652105020/000364.png\n",
            "/frames/00652105020/000428.png\n",
            "/frames/00652105020/000432.png\n",
            "/frames/00652105020/000436.png\n",
            "/frames/00652105020/000448.png\n",
            "/frames/00652105020/000464.png\n",
            "/frames/00652105020/000512.png\n",
            "/frames/00652105020/000520.png\n",
            "/frames/00652105020/000528.png\n",
            "/frames/00652105020/000532.png\n",
            "/frames/00652105020/000596.png\n",
            "/frames/00652105020/000612.png\n",
            "/frames/00652105020/000616.png\n",
            "/frames/00652105020/000680.png\n",
            "/frames/00652105020/000688.png\n",
            "/frames/00652105020/000696.png\n",
            "/frames/00652105020/000700.png\n",
            "/frames/00652105020/000772.png\n",
            "/frames/00652105020/000784.png\n",
            "/frames/00652105020/000788.png\n",
            "/frames/00652105020/000792.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "        print('Encode')\n",
        "        images_features = compute_clip_features(images)\n",
        "        print('Encoded')\n",
        "        j=0\n",
        "        \n",
        "        for imageF in images_features:\n",
        "\n",
        "          if pre_image_feature_set==False:\n",
        "            pre_image_frame=1\n",
        "            pre_image_cv = image[0]\n",
        "            pre_image_feature = imageF\n",
        "            pre_image_feature_set=True\n",
        "\n",
        "          result = 1 - spatial.distance.cosine(pre_image_feature, imageF)\n",
        "          if result <0.85:\n",
        "            objectname = str(pre_image_frame).zfill(6)+'.png'\n",
        "            \n",
        "            ##uploads\n",
        "            upload('frames',productionNumber,objectname,'vframes',pre_image_cv)\n",
        "\n",
        "            #writes to opensearch\n",
        "            featureTimespan = FeatureTimespan(\"\",\"\",\"\",\"\",\"\",0,0,0,np.zeros(512))\n",
        "            featureTimespan.title = administrativeTitle\n",
        "            featureTimespan.prdNo = productionNumber\n",
        "            featureTimespan.url = siteUrl\n",
        "            featureTimespan.sourceUrl = streamUrl\n",
        "            featureTimespan.imageUrl ='https://vframes.s3.eu-north-1.amazonaws.com/frames/'+productionNumber+'/'+objectname\n",
        "            featureTimespan.frameStart =pre_image_frame\n",
        "            featureTimespan.frameEnd = frame[j]-1\n",
        "            featureTimespan.featureFrame = pre_image_frame\n",
        "            featureTimespan.features = pre_image_feature\n",
        "            listfeatureTimespans.append(featureTimespan)\n",
        "            \n",
        "            pre_image_frame=frame[j]\n",
        "            pre_image_cv = convert_from_image_to_cv2(images[j])\n",
        "            pre_image_feature = imageF\n",
        "                    \n",
        "          j=j+1\n",
        "        load_frames_in_index(listfeatureTimespans,\"https://search-video-ovjjz53fbtmpdokttzlfxmkrpu.eu-west-2.es.amazonaws.com\",\"image_search\")\n",
        "        listfeatureTimespans= []\n",
        "        images = []\n",
        "        frame= []"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFMc6BbXfCh0",
        "outputId": "0eca506c-3842-4dc4-fc66-9b36be16e082"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encode\n",
            "Encoded\n",
            "/frames/00652105010/042948.png\n",
            "/frames/00652105010/043016.png\n",
            "/frames/00652105010/043080.png\n",
            "/frames/00652105010/043128.png\n",
            "/frames/00652105010/043168.png\n",
            "/frames/00652105010/043232.png\n",
            "/frames/00652105010/043252.png\n",
            "/frames/00652105010/043292.png\n",
            "/frames/00652105010/043296.png\n",
            "/frames/00652105010/043316.png\n",
            "/frames/00652105010/043344.png\n",
            "/frames/00652105010/043448.png\n",
            "/frames/00652105010/043468.png\n",
            "/frames/00652105010/043504.png\n",
            "/frames/00652105010/043568.png\n",
            "Loading 15 photos...\n",
            "Loaded photos\n"
          ]
        }
      ]
    }
  ]
}